# ðŸ“˜ K-Nearest Neighbors (KNN) â€“ Notes

## 1. Introduction
- **K-Nearest Neighbors (KNN)** is a **non-parametric, lazy learning algorithm**.  
- Used for both:
  - **Classification** â†’ assigns class label based on majority vote of nearest neighbors.  
  - **Regression** â†’ predicts value by averaging nearest neighbors.  

---

## 2. Distance Calculation
KNN depends on finding **nearest neighbors** using a distance metric.

1. **Euclidean Distance** (most common):  
   \[
   d(p, q) = \sqrt{\sum_{i=1}^n (p_i - q_i)^2}
   \]  
   â†’ Straight-line distance between two points.

2. **Manhattan Distance** (city-block distance):  
   \[
   d(p, q) = \sum_{i=1}^n |p_i - q_i|
   \]  
   â†’ Sum of absolute differences.

---

## 3. Hyperparameter: **K**
- **K** = number of neighbors considered.  
- Small \(K\) â†’ more sensitive to noise/outliers.  
- Large \(K\) â†’ smoother decision boundary, but may ignore local patterns.  
- **Selection of K:**
  1. Try different K values.  
  2. Compute error rate (or accuracy).  
  3. Choose K with **lowest error rate / highest accuracy**.  

---

## 4. Error Rate & Model Selection
- For classification:  
  \[
  \text{Error Rate} = \frac{\text{Number of misclassified samples}}{\text{Total samples}}
  \]  
- Plot error vs. K â†’ pick optimal K.  
- Typically, odd K is chosen to avoid ties in binary classification.  

---

## 5. Limitations
- **Works poorly with:**
  - **Outliers** â†’ can distort nearest neighbor calculation.  
  - **Imbalanced datasets** â†’ dominant class outweighs minority class in majority voting.  
- Computationally expensive for large datasets (distance must be calculated for all points).  

---

## 6. Real-Life Applications
- **Recommendation Systems** (finding similar users/items).  
- **Medical Diagnosis** (classifying diseases based on symptoms).  
- **Finance** (predicting loan approval or default risk).  
- **Image Recognition** (classifying objects based on pixel similarity).  
- **Customer Segmentation** (grouping customers with similar behavior).  

---

âœ… **Summary:**  
- KNN is simple, intuitive, and effective for small-to-medium datasets.  
- Choosing **K** and distance metric is critical.  
- Sensitive to outliers & imbalanced data, but widely used in practical ML applications.  
